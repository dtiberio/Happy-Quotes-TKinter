{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happy Quotes - the quotes that brighten your day\n",
    "\n",
    "This script is split into three main sections:\n",
    "\n",
    "1 - A BeautifulSoup based script that scrappes the website \"https://quotes.toscrape.com\" to extract all the quotes and all the authors.\n",
    "\n",
    "2 - A script that calls the API endpoint: \"https://jsonplaceholder.typicode.com/comments?postId=id\" to extract \"made up\" comments for each one of our quotes.\n",
    "\n",
    "3 - A script that transforms all the data extracted in the previous sections and then loads the data into a database.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1\n",
    "Let's start with Beautiful Soup and extract all the data from \"https://quotes.toscrape.com\" about the quotes and the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Constants\n",
    "BASE_URL = \"https://quotes.toscrape.com\"\n",
    "START_URL = BASE_URL + \"/page/1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS4 Locators for the quotes\n",
    "QUOTE_LOCATOR = \"div.quote\"\n",
    "TEXT_LOCATOR = \"span.text\"\n",
    "AUTHOR_LOCATOR = \"small.author\"\n",
    "TAGS_LOCATOR = \"div.tags a.tag\"\n",
    "AUTHOR_LINK_LOCATOR = \"div.quote span a\"\n",
    "NEXT_PAGE_LOCATOR = \"li.next a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS4 Locators for the author details\n",
    "AUTHOR_BORN_DATE_LOCATOR = \"span.author-born-date\"\n",
    "AUTHOR_BORN_LOCATION_LOCATOR = \"span.author-born-location\"\n",
    "AUTHOR_DESCRIPTION_LOCATOR = \"div.author-description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with empty variables\n",
    "# global variables:\n",
    "all_quotes = []\n",
    "authors_urls = set()    # to store unique urls to each author's page\n",
    "all_tags = set()    # to store unique tags\n",
    "all_authors_names = set()    # to store unique authors' names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "\n",
    "def maxSizeOfKey(data_list, key_name):\n",
    "    max_size = 0\n",
    "    for element in data_list:\n",
    "        if len(element[key_name]) > max_size:\n",
    "            max_size = len(element[key_name])\n",
    "    if max_size > 255:\n",
    "        print(f\"Max size of {key_name} is {max_size} characters. Requires a TEXT data type in the database.\")\n",
    "    else:\n",
    "        print(f\"Max size of {key_name} is {max_size} characters. Requires a VARCHAR data type in the database.\")\n",
    "\n",
    "def printAll(data_list, data_name):\n",
    "    print(f\"\\nAll the {data_name}:\")\n",
    "    \n",
    "    for element in data_list:\n",
    "        if isinstance(element, str):\n",
    "            print(element, end=', ')\n",
    "        else:\n",
    "            print(element)\n",
    "        \n",
    "    print(f\"\\nTotal {data_name} = {len(data_list)}\")\n",
    "\n",
    "# Function to extract quotes and update the set of author URLs\n",
    "# the variable quotes is passed by reference, it i updated on each get request\n",
    "\n",
    "def getQuotes(soup, quotes):\n",
    "    \n",
    "    global all_tags, all_authors, authors_urls\n",
    "\n",
    "    # get all the quotes from the current page\n",
    "    quote_elements = soup.select(QUOTE_LOCATOR)\n",
    "    \n",
    "    # go through each single quote in the current page\n",
    "    for quote_element in quote_elements:\n",
    "        text = quote_element.select_one(TEXT_LOCATOR).text\n",
    "        author = quote_element.select_one(AUTHOR_LOCATOR).text\n",
    "        tags = [tag.text for tag in quote_element.select(TAGS_LOCATOR)]\n",
    "        author_link = BASE_URL + quote_element.select_one(AUTHOR_LINK_LOCATOR).attrs['href']\n",
    "        \n",
    "        # update, by reference, the global variable, all_quotes, which is a list of dicts\n",
    "        quotes.append({\n",
    "            'text': text,\n",
    "            'author': author,\n",
    "            'tags': tags\n",
    "        })\n",
    "        \n",
    "        # update the global variable, all_tags, which is a set of unique elements\n",
    "        for tag in tags:\n",
    "            all_tags.add(tag)\n",
    "        \n",
    "        # update the global variable, all_authors_names, which is a set of unique elements\n",
    "        all_authors_names.add(author)\n",
    "        \n",
    "        # update the global variable, authors_urls, which is a set of unique elements\n",
    "        authors_urls.add(author_link)\n",
    "\n",
    "# Function to extract the details of each author \n",
    "def getAuthors():\n",
    "    \n",
    "    global authors_urls\n",
    "    # start with an empty list\n",
    "    authors = []\n",
    "    \n",
    "    # go through each of authors_urls and get their details\n",
    "    for author_url in authors_urls:\n",
    "        page = requests.get(author_url)\n",
    "\n",
    "        # add error message if request fails\n",
    "\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        # get the details of each author\n",
    "        author_name = soup.select_one(\"h3.author-title\").text.strip()\n",
    "        born_date = soup.select_one(AUTHOR_BORN_DATE_LOCATOR).text\n",
    "        born_location = soup.select_one(AUTHOR_BORN_LOCATION_LOCATOR).text\n",
    "        description = soup.select_one(AUTHOR_DESCRIPTION_LOCATOR).text.strip()\n",
    "        \n",
    "        # create a list of dicts\n",
    "        authors.append({\n",
    "            'name': author_name,\n",
    "            'born_date': born_date,\n",
    "            'born_location': born_location,\n",
    "            'description': description\n",
    "        })\n",
    "    \n",
    "    # return a list of dicts, with the details of all the authors\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main scraping logic\n",
    "page_url = START_URL\n",
    "\n",
    "# first, get all the quotes\n",
    "while page_url:     # stops when page_url = None\n",
    "    page = requests.get(page_url)\n",
    "    \n",
    "    # add error message if request fails\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # global variables are passed by reference, all_quotes is updated on each get request\n",
    "    getQuotes(soup, all_quotes)\n",
    "    \n",
    "    # get next page url\n",
    "    next_page_element = soup.select_one(NEXT_PAGE_LOCATOR)  \n",
    "    page_url = BASE_URL + next_page_element.attrs['href'] if next_page_element else None    # None -> no more pages\n",
    "\n",
    "# next, let's get all the authors\n",
    "all_authors = getAuthors()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python variable - all_quotes\n",
    "\n",
    "![all_quotes](img\\var_all_quotes.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final result\n",
    "printAll(all_quotes, 'quotes')\n",
    "\n",
    "# check which data type we need to use in the database for key 'text'\n",
    "print(\"Which database data type do we need for quote['text']?\")\n",
    "maxSizeOfKey(all_quotes, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python variable - all_authors\n",
    "![all_authors](img\\var_all_authors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final result\n",
    "printAll(all_authors, 'authors')\n",
    "\n",
    "# check which data type we need to use in the database for key 'text'\n",
    "print(\"Which database data type do we need for author['description']?\")\n",
    "maxSizeOfKey(all_authors, 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the variables: all_tags and all_authors_names\n",
    "# convert the sets into ordered lists and check the required data type for the database\n",
    "\n",
    "all_tags = sorted(all_tags) # a list of all the tags\n",
    "printAll(all_tags, 'tags')\n",
    "print(\"Which database data type do we need for 'all_tags'?\", end=' ')\n",
    "print(\"Use TEXT data type in the database.\" if sum(len(tag) for tag in all_tags) > 255 else \"Use VARCHAR data type in the database.\")\n",
    "print(\"Total lenght = \",sum(len(tag) for tag in all_tags))\n",
    "\n",
    "all_authors_names = sorted(all_authors_names) # a list of all the authors' names\n",
    "printAll(all_authors_names, 'authors_names')\n",
    "print(\"Which database data type do we need for 'all_authors_names'?\", end=' ')\n",
    "print(\"Use TEXT data type in the database.\" if sum(len(author) for author in all_authors_names) > 255 else \"Use VARCHAR data type in the database.\")\n",
    "print(\"Total lenght = \", sum(len(author) for author in all_authors_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables \"all_quotes\" and \"all_authors\" contain all the data that we extracted from the \"https://quotes.toscrape.com\" website.  \n",
    "The variables \"all_tags\" and \"all_authors_names\" contain lists with metadata, the unique values of each tag and each author's name.\n",
    "\n",
    "# Section 1 - ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2\n",
    "We'll now extract 100 comments (to match the 100 quotes) from the API endpoint: \n",
    "\"https://jsonplaceholder.typicode.com/comments?postId=id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the API settings\n",
    "\n",
    "# import requests was completed at the start\n",
    "\n",
    "# 1 - define the API endpoint\n",
    "endpoint = \"https://jsonplaceholder.typicode.com/comments\"\n",
    "\n",
    "# 2 - define the parameters\n",
    "parameters = {\n",
    "    'postId' : None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API functions\n",
    "\n",
    "def fetchPostComments(post_id):\n",
    "    \n",
    "    # API endpoint updated with post_id params\n",
    "    global endpoint, parameters\n",
    "    parameters['postId'] = post_id\n",
    "\n",
    "    # 3 - call the API with updated params\n",
    "    response = requests.get(url=endpoint, params=parameters)\n",
    "    \n",
    "    # testing error messages\n",
    "    # if post_id in (1, 10, 20, 40, 50, 60, 70, 80, 90, 100):\n",
    "    #     print(f\"ERROR: Failed to fetch comments for post ID {post_id}\")\n",
    "    #     return [f\"ERROR: Failed to fetch comments for post ID {post_id}\"]\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # 4 - return the response from the API\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"ERROR: Failed to fetch comments for post ID {post_id}\")\n",
    "        return [f\"ERROR: Failed to fetch comments for post ID {post_id}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_comments = []\n",
    "\n",
    "# total quotes fetched = len(all_quotes) = 100\n",
    "for post_id in range(1, len(all_quotes) + 1):\n",
    "    comments = fetchPostComments(post_id)\n",
    "    all_comments.extend(comments)\n",
    "\n",
    "print(f\"Total number of comments fetched: {len(all_comments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python variable - all_comments\n",
    "![all_comments](img\\var_all_comments.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all_comments\n",
    "printAll(all_comments, 'comments')\n",
    "\n",
    "# check which database data type is required for key 'body'\n",
    "maxSizeOfKey(all_comments, 'body')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable \"all_comments\" includes all the comments extracted from the API endpoint, that's 5 comments for each one of 100 quotes, so that's a total of 500 comments.\n",
    "\n",
    "This completes the \"Extract\" phase of the script.\n",
    "\n",
    "# Section 2 - ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3\n",
    "This section of the script Transforms the extracted data and Loads it into a SQL database.\n",
    "  \n",
    "1. Use dotenv to import the db access details and the name of the SQL init script\n",
    "2. Create a database from the SQL init script\n",
    "3. Import a Python module with the Pydantic based classes to interact with the database\n",
    "4. Clean up the data, Transform the variables being used, Check the data types, Map the data to Python classes:\n",
    "    1. all_quotes = contains all the  details about each quote \n",
    "    2. all_authors = contains all the  details about the authors\n",
    "    3. all_comments = contains all the comments for each quote\n",
    "5. Use the Pydantic classes to load the data into the SQL database\n",
    "\n",
    "<p style=\"color:red; font-size:20px;\"><strong>Beware! Update the settings in the file .env to match your environment!</strong></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the database and models classes\n",
    "\n",
    "from happy_models import *\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection to the database server\n",
    "# Beware! .env default settings are: host = localhost, user = root, pwd = '', port = 3307\n",
    "# You must update the .env file with the settings for your environment!\n",
    "\n",
    "# set the name for your database\n",
    "# we do that here, as MySQLDB() class allows one class instance for each database\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "DB_NAME = os.getenv('DATABASE_NAME')\n",
    "# create an object of MySQLDB class to manage your database\n",
    "db = MySQLDB(DB_NAME)\n",
    "db.test_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create these tables in the database\n",
    "![database schema](img\\database_schema.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this once to create the database and the tables\n",
    "# if the database and tables already exist, then they are not overwritten\n",
    "\n",
    "# during testing, keep overwrite = True, to start from a new DB on each test run\n",
    "db.create_database(True)\n",
    "\n",
    "author_sql =\"\"\"\n",
    "-- Create the author table\n",
    "CREATE TABLE author (\n",
    "    id_author INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    date_created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n",
    "    date_modified TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "    name VARCHAR(255) NOT NULL,\n",
    "    birth_date DATE,\n",
    "    birth_city VARCHAR(255),\n",
    "    birth_state VARCHAR(255),\n",
    "    birth_country VARCHAR(255),\n",
    "    description TEXT\n",
    ");\n",
    "\"\"\"\n",
    "db.create_table('author', author_sql, False)\n",
    "\n",
    "quote_sql=\"\"\"\n",
    "-- Create the quote table\n",
    "CREATE TABLE quote (\n",
    "    id_quote INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    date_created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n",
    "    date_modified TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "    content TEXT NOT NULL,\n",
    "    author_id INT NOT NULL,\n",
    "    tags VARCHAR(255),\n",
    "    FOREIGN KEY (author_id) REFERENCES author(id_author)\n",
    ");\n",
    "\"\"\"\n",
    "db.create_table('quote', quote_sql, False)\n",
    "\n",
    "comment_sql=\"\"\"\n",
    "-- Create the comment table\n",
    "CREATE TABLE comment (\n",
    "    id_comment INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    date_created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n",
    "    date_modified TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "    quote_id INT NOT NULL,\n",
    "    title VARCHAR(255) NOT NULL,\n",
    "    details TEXT,\n",
    "    user_email VARCHAR(255),\n",
    "    FOREIGN KEY (quote_id) REFERENCES quote(id_quote)\n",
    ");\n",
    "\"\"\"\n",
    "db.create_table('comment', comment_sql, False)\n",
    "\n",
    "metadata_sql=\"\"\"\n",
    "-- Create the metadata table\n",
    "CREATE TABLE metadata (\n",
    "    id_key INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    date_created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n",
    "    date_modified TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "    key_name VARCHAR(255) NOT NULL,\n",
    "    key_value TEXT NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "db.create_table('metadata', metadata_sql, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's clean up and transform the data\n",
    "We'll be mapping the Python variables to Pydantic classes and then save the data in a SQL database.\n",
    "![Map of the Pydantic Classes](img\\oop_classes.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with the 'author' table, because it doesn't have any Foreign Keys\n",
    "\n",
    "for author in all_authors:\n",
    "    name = author['name']\n",
    "    dob = datetime.strptime(author['born_date'], \"%B %d, %Y\")   # convert string to date\n",
    "    description = author['description']\n",
    "    born_location = author['born_location'][2:]     # strips the characters 'in ' from the start of the string\n",
    "    location_list = born_location.split(', ')       # creates a list of places\n",
    "\n",
    "    match len(location_list):       # allocate the right place to the right location\n",
    "        case 0:\n",
    "            birth_city = None\n",
    "            birth_state = None\n",
    "            birth_country = None\n",
    "        case 1:\n",
    "            birth_city = None\n",
    "            birth_state = None\n",
    "            birth_country = location_list[0]\n",
    "        case 2:\n",
    "            birth_city = location_list[0]\n",
    "            birth_state = None\n",
    "            birth_country = location_list[1]\n",
    "        case 3:\n",
    "            birth_city = location_list[0]\n",
    "            birth_state = location_list[1]\n",
    "            birth_country = location_list[2]\n",
    "        case _:\n",
    "            birth_city = location_list[0]\n",
    "            birth_state = location_list[1]\n",
    "            birth_country = location_list[2]\n",
    "    \n",
    "    # leverage Pydantic type annotations and validations to do type checking and coercion if needed\n",
    "    author_obj = Author(\n",
    "        name = name,\n",
    "        birth_date = dob,\n",
    "        birth_city = birth_city,\n",
    "        birth_state = birth_state,\n",
    "        birth_country = birth_country,\n",
    "        description = description\n",
    "    )\n",
    "\n",
    "    # save each author to the database\n",
    "    author_obj.db_save(db)\n",
    "\n",
    "    # print each object to confirm that an id was assigned to it\n",
    "    print(author_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data. \n",
    "# The 'quote' table, has one Foreign Key from 'author', so let's check that quote['author'] == author['name'].\n",
    "# If there are any misspellings, then we correct quote['author']\n",
    "\n",
    "# List with all author names extracted from the quotes\n",
    "print(\"Total names_from_quotes: \", len(all_authors_names))\n",
    "\n",
    "# List with all author names extracted from the author page\n",
    "names_from_authors = [author['name'] for author in all_authors]\n",
    "print(\"Total names_from_authors: \", len(names_from_authors))\n",
    "\n",
    "# Let's find the differences between the two lists\n",
    "\n",
    "# Convert lists to sets\n",
    "set_quotes = set(all_authors_names)\n",
    "set_authors = set(names_from_authors)\n",
    "\n",
    "# Find names in quotes but not in authors\n",
    "names_in_quotes_not_in_authors = set_quotes - set_authors\n",
    "\n",
    "# Find names in authors but not in quotes\n",
    "names_in_authors_not_in_quotes = set_authors - set_quotes\n",
    "\n",
    "# Print the results\n",
    "print(\"Names in quotes but not in authors:\", names_in_quotes_not_in_authors)\n",
    "print(\"Names in authors but not in quotes:\", names_in_authors_not_in_quotes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data. \n",
    "# Let's replace all occurences of 'Alexandre Dumas fils' in quotes with 'Alexandre Dumas-fils'\n",
    "# Beware! If you've run this script before, then you will no longer find these differences in the data.\n",
    "\n",
    "i = 0\n",
    "for quote in all_quotes:\n",
    "    if quote['author'] == 'Alexandre Dumas fils':\n",
    "        quote['author'] = 'Alexandre Dumas-fils'\n",
    "        i += 1\n",
    "print(f\"Completed {i} replacements\")\n",
    "\n",
    "# update list of all_authors_names, it will be saved in the database later\n",
    "all_authors_names = sorted(names_from_authors)\n",
    "print(\"Total authors' names: \", len(all_authors_names))\n",
    "print(all_authors_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 'quote' table, has one Foreign Key from 'author'\n",
    "\n",
    "for quote in all_quotes:\n",
    "    \n",
    "    # Slicing to remove the first and last characters, they're unicode codes that we don't want\n",
    "    content = quote['text'][1:-1]     \n",
    "        \n",
    "    tags = quote['tags']\n",
    "\n",
    "    # fetch author_id from the database\n",
    "    sql_query = \"\"\"\n",
    "    SELECT id_author FROM author WHERE name = %s\n",
    "\"\"\"\n",
    "    author_name = quote['author']\n",
    "    author_id = db.sql_query(sql_query, (author_name, ))[0]['id_author']\n",
    "    \n",
    "    quote_obj = Quote(\n",
    "        content = content,\n",
    "        author_id = author_id,\n",
    "        tags = tags\n",
    "    )\n",
    "\n",
    "    quote_obj.db_save(db)\n",
    "    print(quote_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'comment' table, has one Foreign Key from 'quote'\n",
    "# Let's check that comment.quote_id <= Max(quote.id_quote)\n",
    "\n",
    "sql_query = \"\"\"SELECT MAX(id_quote) AS max_id_quote\n",
    "FROM quote\"\"\"\n",
    "max_quote_id = db.sql_query(sql_query, ())[0]['max_id_quote']\n",
    "\n",
    "# Initialize the flag\n",
    "error_found = False\n",
    "for comment in all_comments:\n",
    "    if comment['postId'] <= max_quote_id:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Error in comment with postId = \", comment['postId'])\n",
    "        error_found = True\n",
    "if not error_found:\n",
    "    print(\"No errors found in comments\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save all the comments in the database with the Foreign Key from 'quote'\n",
    "\n",
    "for comment in all_comments:\n",
    "    quote_id = comment['postId']\n",
    "    title = comment['name']\n",
    "    details = comment['body']\n",
    "    user_email = comment['email']\n",
    "\n",
    "    # instantiate the comment object\n",
    "    comment_obj = Comment(\n",
    "        quote_id = quote_id,\n",
    "        title = title,\n",
    "        details = details,\n",
    "        user_email = user_email\n",
    "    )\n",
    "\n",
    "    # save the comment object to the database\n",
    "    comment_obj.db_save(db)\n",
    "    print(comment_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, let's save the Metadata, it holds some details that we can use later\n",
    "\n",
    "metadata_tags = Metadata(\n",
    "    key_name = 'all_tags',\n",
    "    key_value = all_tags\n",
    ")\n",
    "\n",
    "metadata_tags.db_save(db)\n",
    "print(metadata_tags)\n",
    "\n",
    "metadata_authors = Metadata(\n",
    "    key_name = 'all_authors',\n",
    "    key_value = all_authors_names\n",
    ")\n",
    "\n",
    "metadata_authors.db_save(db)\n",
    "print(metadata_authors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script ends here.  \n",
    "We've used BeautifulSoup4 to webscrape one website, then consummed one API to get mock-data for our comments.  \n",
    "We've used the Pydantic library to leverage its type annotations and validation features before loading the data to a SQL database.  \n",
    "The configuration settings for the SQL database are stored in a .env file, for added security.  \n",
    "\n",
    "We now have a database,\"happy_quotes\", that contains four tables:  \n",
    "1. \"author\" - with all the details about each author\n",
    "2. \"quote\" - with all the details about each quote and a Foreign Key into the \"author\" table\n",
    "3. \"comment\" - with all the details about each comment and a Foreign Key into the \"quote\" table\n",
    "4. \"metadata\" - with two lists, 'all_tags' and 'all_authors'\n",
    "  \n",
    "This completes the \"Transform\" and \"Load\" phases of the script.  \n",
    "  \n",
    "# Section 3 - ends here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
